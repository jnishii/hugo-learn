---
title: 形式ニューロンと活性化関数
weight: 10
---

## McCulloch & Pittsによる神経細胞モデル

McCulloch & Pittsが1943年に提案した神経細胞の入出力モデル([形式ニューロン](https://ja.wikipedia.org/wiki/%E5%BD%A2%E5%BC%8F%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3))は以下の通り。

<div>
$$
\begin{align}
y&=f(x-\theta)\\
x&=\sum_{i=1}^Nw_iy_i
\end{align}
$$
</div>

ここで，$x$と$y$はそれぞれ素子(神経細胞)の内部状態と出力,
$y_i$と$w_i$は第$i$素子からの入力信号と，入力信号による影響の大きさを決める結合荷重, $f:x\to y$は
[活性化関数](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#%E7%B7%9A%E5%BD%A2%E7%B5%90%E5%90%88%EF%BC%88%E5%8D%98%E7%B4%94%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%EF%BC%89)。

McCulloch & Pittsは活性化関数としてステップ関数を用いたが，今日では後述のように他の様々な関数が利用される。その点を除くと，この形式ニューロンは現在の深層学習でも広く使われている。

## パーセプトロン

[パーセプトロン](https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3)はRosenblattにより1958年に提案された，形式ニューロンを用いた脳の情報処理モデル。
Perceptronの論文は[こちら](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)。

- **単純パーセプトロン**:
  - 入力層と出力層の2層のみのもの(形式ニューロンの配置は出力層のみの1層)
- **パーセプトロン**:
  - 入力層と出力層の他，情報連合を行う中間層を持つもの
  - Rosenblattは再帰結合を持つ構造についても言及している
- [多層パーセプトロン](https://ja.wikipedia.org/wiki/%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3) (MultiLayer Perceptron, MLP):
  - Rosenblattは活性化関数としてステップ関数を用い，分類問題を解く脳モデルとしてパーセプトロンを議論した。現在は，入力層と出力層の間に中間層をもつ，順伝播型のニューラルネットワークを一般に多層パーセプトロンと呼び，活性化関数には様々な関数が用いて，分類だけでなく回帰問題にも利用される。

## 活性化関数の種類

現在は様々な活性化関数が目的に応じて使われている。

### ステップ関数(step function)

<div>
$$
f(x)=
\begin{cases}
1 & (x\ge\theta)\\
0 & (otherwise)
\end{cases}
$$
</div>

ただし，$\theta$ はしきい値。

**特徴**
- McCulloch & Pittsにより1943年に提案された神経細胞の入出力モデル
- 1素子で2入力AND回路やOR回路を構成可能
- 多層化すればXOR等線形分離不可能な分類問題も扱える
<!-- - チューリングマシンと同等の計算能力をもつ-->

**欠点**
- $x\ne 0$で$f'(x)=0$, $x=0$で微分不可能。
[勾配降下法](../learning)は適用しようとしても，$x\ne 0$では勾配が0 ($f'(x)=0$), $x=0$で微分不可能。したがって勾配降下法は適用できない(中間層の学習方法が未発見だったことが第一次ニューラルネットワークブーム終焉の理由の一つ)  
- 多クラス分類問題では，同時に複数の出力値が1になりえる。この時推定クラスを一意に絞る方法がない。

### 線形関数(linear function)

$$
f(x)=ax
$$

**特徴**
- 連続関数の学習可能
- 多クラス分類も可能(複数の出力に対して, maxやsoftmaxで決定可能)
- 回帰を目的とするネットワークの出力層にしばしば使われる

**欠点**
- 線形関数しか表現できない。多層化しても，結局二層(入力層+出力層)のネットワークと等価なものしかできない


### シグモイド関数(sigmoid function)
$$
f(x)=\frac{1}{1+e^{-x}}
$$

**利点**
- 実際の神経細胞の発火率(単位時間あたりの出力インパルス数)が，入力の大きさに対してシグモイド状の変化を示すことをモデル化
- スラップ関数を微分可能な非線形関数に置き換えたことで，多層パーセプトロンに対して勾配降下法が利用可能に(第二次ニューラルネットワークブームのきっかけの一つ)
- 3層ネットワークで任意の連続な非線形関数を近似可能

**欠点**
- 入力の大きさ$|x|$が大きいと，$f(x)$の勾配が小さくなり，学習が進まなくなることが有る(**勾配消失**)

### 正規化線形関数 (Rectified Linear function)

$$
f(x) = \max(0,x)
$$

- この関数を活性化関数とする素子をReLU(Rectified Linear Unit)と呼ぶ。
- ランプ関数とも呼ばれる

**利点**
- シグモイド関数に比べて計算時間が短い
- 非線形関数の近似可能
- シグモイド関数のように，目的に応じて出力の値域を調整する必要がない
- 負の状態では出力が0になるため，ネットワーク活性をsparseにできる
- 勾配消失問題が，経験的に解消される

## Kerasで使える活性化関数

- [活性化関数の使い方(Keras)](https://keras.io/ja/activations/)
