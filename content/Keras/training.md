---
title: 学習方法
weight: 30
---


## 学習頻度に関する基本用語

- **エポック数(epochs)**
  - 1つの訓練データを繰り返して学習させる回数
  - 少ないと学習が不十分に
  - 多すぎると過学習に
- **バッチサイズ(batch_size)**
  - ミニバッチ(ランダムに抽出した訓練データ)のサンプルサイズ
  - 2^nにすることが多い
  - 小さくすると
    - メモリ使用量が減る(計算量は増える)
    - 個々のデータに対するパラメータの変化が大きくなるので，一部の異常データによる影響も受けやすくなる
  - 大きくすると
    - 並列計算を利用しやすくなる
    - データの平均的性質が反映されるが，局所解にはまると学習が進まなくなりやすい。
    - 訓練データセットに似たサンプルが多く存在するときには，無駄に学習を多く行うことになる

## 学習方法

多層パーセプトロンの学習を議論する時，活性化関数の閾値は，常に1の大きさの入力信号に対する閾値で置き換えることで，結合荷重のみを学習パラメータとして議論することができる。

学習(訓練)においては，誤差関数$E(\boldsymbol{w})$を最小化するようにパラメータ($\boldsymbol{w}$)を更新する。
勾配降下法を用いると次式のようになる。

$$
\begin{align}
\boldsymbol{w}&\leftarrow\boldsymbol{w}-\epsilon\nabla E(\boldsymbol{w})\\\\\\
\nabla E&=\frac{\partial E}{\partial\boldsymbol{w}}
\end{align}
$$


例えば，平均二乗誤差を用いる場合は$\displaystyle E(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^N ||\boldsymbol{d}_n-\boldsymbol{y}_n||^2$であり，右辺の$\boldsymbol{y}_n$は陽に$\boldsymbol{w}$を使って書き直せるので， 各素子の入出力を規定する関数$y=\boldsymbol{f}(\boldsymbol{x};\boldsymbol{w})$が微分可能な関数(sigmoid関数など)であれば，解析的に$\boldsymbol{w}$に対する微分計算が可能。各入力値に対する$\nabla E$を計算することでパラメータ更新をできる。


訓練データに対してパラメータ更新をどの程度の頻度で行うかによって，以下のように分類される。

- **バッチ勾配降下法**
  - 目的関数(コスト関数)の勾配を全サンプルに対して計算し，その総和を使ってパラメータ更新を行う
  - 「学習の回数=**エポック数**」になる
  - 処理速度が遅い
  - オンライン更新はできない
  - 局所解にはまると抜けられない
- **確率的勾配降下法**(Stochastic Gradient Descent, SGD)
  - 目的関数(コスト関数)の勾配をサンプル毎に計算し，それを使ってパラメータ更新を行う
  - バッチ勾配降下法より学習速度は大抵速い
  - 目的関数の値は学習に伴って頻繁に変動する(短期的には悪くなることも)
  - 局所解に陥っても抜け出せる場合がある
  - 最適解への完全な収束は難しい
- **ミニバッチ勾配降下法**
  - こちらも[確率的最急降下法(Stochastic gradient descent, SGD)](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)と呼ぶことは多い
  - サンプル中の一部をランダムに抜き取ってミニバッチとし，それに対してバッチ学習を行う

### 確率的勾配法の修正版

最急降下法には，学習率の調整を加えた様々な修正版がある。以下は例。

- **AdaGrad**:
  - 勾配のニ乗量を使って，学習率を徐々に減衰させる
  - 局所解に陥るのを防ぐ効果を期待できるが，一方で時間とともに学習が停滞する
- **RMSprop**:
  - 勾配のニ乗の指数移動平均を使い，勾配の減衰に伴う学習の停滞を防ぐ
- **AdaProp**:
  - AdaGradとRMSpropを組み合わせた方法
- **Adam**:
  - AdaGrad, RMSprop, AdaPropの変形版

### Kerasで使える学習方法

- [オプティマイザの利用方法(Keras)](https://keras.io/ja/optimizers/)

## 学習テクニック

- **ドロップアウト**(Dropout)
  - 設定した確率で選択した結合荷重は，更新をしない
  - 過学習を防ぐ効果あり

## 参考URL

- [勾配降下法(wikipedia)](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)
- [勾配降下法の最適化アルゴリズムを概観する](http://postd.cc/optimizing-gradient-descent/#gradientdescentvariants)
